\documentclass[11pt]{article}

\newcommand{\yourname}{Kevin Zhang}

\def\comments{0}

%format and packages

%\usepackage{algorithm, algorithmic}
\usepackage{forest}
\usepackage{tikz}
\usepackage{algpseudocode}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{tcolorbox}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{framed}
\usepackage{verbatim}
\usepackage[margin=1.0in]{geometry}
\usepackage{microtype}
\usepackage{kpfonts}
\usepackage{palatino}
	\DeclareMathAlphabet{\mathtt}{OT1}{cmtt}{m}{n}
	\SetMathAlphabet{\mathtt}{bold}{OT1}{cmtt}{bx}{n}
	\DeclareMathAlphabet{\mathsf}{OT1}{cmss}{m}{n}
	\SetMathAlphabet{\mathsf}{bold}{OT1}{cmss}{bx}{n}
	\renewcommand*\ttdefault{cmtt}
	\renewcommand*\sfdefault{cmss}
	\renewcommand{\baselinestretch}{1.06}

\usepackage[boxruled,vlined,nofillcomment]{algorithm2e}
	\SetKwProg{Fn}{Function}{\string:}{}
	\SetKwFor{While}{While}{}{}
	\SetKwFor{For}{For}{}{}
	\SetKwIF{If}{ElseIf}{Else}{If}{:}{ElseIf}{Else}{:}
	\SetKw{Return}{Return}
	

%enclosure macros
\newcommand{\paren}[1]{\ensuremath{\left( {#1} \right)}}
\newcommand{\bracket}[1]{\ensuremath{\left\{ {#1} \right\}}}
\renewcommand{\sb}[1]{\ensuremath{\left[ {#1} \right\]}}
\newcommand{\ab}[1]{\ensuremath{\left\langle {#1} \right\rangle}}

%probability macros
\newcommand{\ex}[2]{{\ifx&#1& \mathbb{E} \else \underset{#1}{\mathbb{E}} \fi \left[#2\right]}}
\newcommand{\pr}[2]{{\ifx&#1& \mathbb{P} \else \underset{#1}{\mathbb{P}} \fi \left[#2\right]}}
\newcommand{\var}[2]{{\ifx&#1& \mathrm{Var} \else \underset{#1}{\mathrm{Var}} \fi \left[#2\right]}}

%useful CS macros
\newcommand{\poly}{\mathrm{poly}}
\newcommand{\polylog}{\mathrm{polylog}}
\newcommand{\zo}{\{0,1\}}
\newcommand{\pmo}{\{\pm1\}}
\newcommand{\getsr}{\gets_{\mbox{\tiny R}}}
\newcommand{\card}[1]{\left| #1 \right|}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\negl}{\mathrm{negl}}
\newcommand{\eps}{\varepsilon}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\eqand}{\qquad \textrm{and} \qquad}
\newcommand{\ind}[1]{\mathbb{I}\{#1\}}
\newcommand{\sslash}{\ensuremath{\mathbin{/\mkern-3mu/}}}
\newcommand{\pipe}{\hspace{3pt}|\hspace{3pt}}

%mathbb
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
%mathcal
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}

%theorem macros
\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{fact}[thm]{Fact}
\newtheorem{clm}[thm]{Claim}
\newtheorem{rem}[thm]{Remark}
\newtheorem{coro}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{conj}[thm]{Conjecture}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheoremstyle{case}{}{}{}{}{}{:}{ }{}
\theoremstyle{case}
\newtheorem{case}{Case}

\theoremstyle{theorem}
\newtheorem{prob}{Problem}
\newtheorem{sol}{Solution}

\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

\begin{document}
{\large
\noindent Name: \yourname}

\vspace{15pt}

% Problem 1
\begin{prob}
\end{prob}

\begin{enumerate}[label=(\alph*)]

% Part A
\item The probability that $h(A) = h(B)$ is $J(A,B)$ or $\frac{|A \cap B|}{|A \cup B|}$. This is somewhat intuitive. 
The hash function is $h(A) = min_{x \in A} \pi (x)$ where $\pi$ is a uniformly random permutation over the dictionary $|U| = n$.
In order for $h(A) = h(B)$, the mininium $\pi$ value must be the same. In other words, the word must be same across $A$ and $B$.
The number of shared words is $|A \cap B|$. The number of total possible attempts is $|A \cup B|$. 
Therefore, $Pr[h(A) = h(B)] = \frac{|A \cap B|}{|A \cup B|}$.

% Part B
\item We have $k$ independent hash functions $h_1, h_2, ..., h_k$ and for documents $A$ and $B$, we can store $h_1(A), ..., h_k(A)$
and $h_1(B), ..., h_2(B)$. We want to devise an algorithm to produce estimate $Z$ from the stored hashes such that

\[
  \text{Pr}[|Z - J(A,B)| \geq \epsilon] \leq 1/3
\]

Let $Z_i$ be an indicator variable whether $h_i(A) = h_i(B)$. Then, we have 

\begin{align*}
  \mathbb{E}[Z_i] &= Pr[h_i(A) = h_i(B)] \\
                  &= J(A,B) \\
  \text{Var}(Z_i) &\leq 1 \emph{    this is generally true for indicator variable}
\end{align*}

With $k$ hash functions, we can produce $Z$ as the mean of these indicators. $Z = \frac{Z_1 + Z_2 + ... + Z_k}{k}$.
We can express the expected value and variance of $Z$ in terms of $Z_i$. Specifically, $\mathbb{E}[Z] = \mathbb{E}[Z_i]$,
and $\text{Var}(Z) = \text{Var}(Z_i)/k$.

\begin{align*}
  \mathbb{E}[Z] &= \mathbb{E}[Z_i] \\
                &= J(A,B) \\
  \text{Var}(Z) &= \text{Var}(Z_i)/k \\
                &\leq 1/k
\end{align*}

We can then plug into Chebyshev's Inequality. Furthermore, we can use $k = 3\epsilon^2$. 

\begin{align*}
  \text{Pr}[|Z - \mathbb{E}[Z]| \geq \epsilon] &\leq \frac{\sigma^2}{\epsilon^2} \\
  \text{Pr}[|Z - J(A,B)| \geq \epsilon] &\leq \frac{1}{3} \\
\end{align*}

\end{enumerate}

\vspace{20px}

% Problem 2
\begin{prob}
\end{prob}

\begin{enumerate}[label=(\alph*)]

% Part A
\item The problem can be expressed as an LP:

\begin{align*}
  \text{min} \frac{1}{n} \sum_{i}^{n} z_i \\
  y_i -a^Tx_i - b \leq z_i \forall i \\
  -y_i + a^Tx_i + b \leq z_i \forall i \\
  z_i \geq 0
\end{align*}

% Part B
\item Code Below. The average error per test example was 0.509512.
\begin{verbatim}
import csv
import itertools
from cvxopt import matrix
from cvxopt.modeling import op, variable, sum, min, dot

x = []
y = []
with open('winequality-red.csv', newline='') as input:    
    reader = csv.reader(input, delimiter=';')
    for row in itertools.islice(reader, 1, 1501):
        nums = list(map(float, row))
        x.append(nums[:-1])
        y.append(nums[-1])

# d is the dimension of a
d = len(x[0])
# n is the number of training examples
n = len(x)

# Variables
a = variable(11)
b = variable()
z = variable()

# Value Matricies
X = matrix(x)
Y = matrix(y)

# Constraints
c1 = (Y - dot(a,X) - b <= z)
c2 = (-Y + dot(a,X) + b <= z)
c3 = (-z <= 0)

# LP problem
lp = op(min(sum(z)), [c1, c2, c3])
lp.solve()

# test with the computed a and b
total_error = 0.0
count = 0
with open('winequality-red.csv', newline='') as input:
    reader = csv.reader(input, delimiter=';')
    for row in itertools.islice(reader, 1501, None):
        count += 1
        nums = list(map(float, row))
        x = nums[:-1]
        y = nums[-1]
        error = b.value[0]-y
        for i in range(d):
            error += x[i] * a.value[i]
        total_error += abs(error)

print("Average error per test example is %f" % (total_error/count))

\end{verbatim}

\end{enumerate}

% Problem 3
\begin{prob}
\end{prob}

\begin{enumerate}[label=(\alph*)]

% Part A
\item Let $x_{i,j}$ indicate whether edge from $i \in X$ to $j \in Y$ is selected. Then,
we can formulate the maximum set of edges such that no two edges share common endpoint as a LP:

\begin{align*}
  \text{max} \sum_{i \in X, j \in Y} x_{i,j} \\
  \sum_{i \in X} x_{i,j} = 1 \forall j \\
  \sum_{j \in Y} x_{i,j} = 1 \forall i \\
  0 \leq x_{i,j} \leq 1 
\end{align*}

% Part B
\item The dual of the problem above can be expressed as an idea: The minimum number of edges
that can be removed (if every vertex is connected) so that no vertex is shared. If we let
$y_{i,j}$ indicate whether we remove an edge $i \in X$ to $j \in Y$, then we can express the dual LP below.
The thought process is that the final number of connected edges to a vertex $i$ or $j$ must be $\leq 1$.

\begin{align*}
  \text{min} \sum_{i \in X, j \in Y} y_{i,j} \\
  \sum_{i \in X} 1 - \sum_{i \in X} y_{i,j} \leq 1 \forall j \\
  \sum_{j \in Y} 1 - \sum_{i \in Y} y_{i,j} \leq 1 \forall i \\
  0 \leq y_{i,j} \leq 1
\end{align*}

% Part C
\item Left Blank For Now

\end{enumerate}

% Problem 4
\begin{prob}
\end{prob}

\begin{enumerate}[label=(\alph*)]

% Part A
\item We have a set of elements $V = \{e_1, e_2, ..., e_n\}$ and $m$ subsets $S_1, S_2, ..., S_m$. 
Each set has weight $w_i \geq 0$. We want to find a collection of subsets that covers all of $V$ and
minimizes the total weight. If we let $x_i$ indicate whether a subset $S_i$ is selected, we can
frame this as an LP:

\begin{align*}
  \text{min} \sum_{i=1}^m w_i * x_i \\
  \sum_{e \in S_i} x_i \geq 1 \forall e \in V \\
  0 \leq x_i \leq 1   
\end{align*}

% Part B
\item Suppose we have a fractional solution to the LP above. We can round the solution to
produce an integral solution. This is accomplished as follows: the set $S_i$ is picked with
probability of min(1, 2$x_i$ln$n$). We want to show that the probability that an element $e_j$ is covered
is at least $1 - \frac{1}{n^2}$.

We can start by examining the possibility that $e_j$ is not selected at all:

\[
  \text{Pr[$e_j$ not selected]} = \prod_{e_j \in S_i} ( 1 - \text{min}(1, 2x_i\text{ln}n))
\]

The min is problematic, so we can use case analysis. In case 1, we have $2x_i\text{ln}n \geq 1$. 
Then the expression reduces to: 

\[
  \text{Pr[$e_j$ not selected]} = \prod_{e_j \in S_i} ( 1 - 1 ) = 0 
\]

In case 2, we have $2x_i\text{ln}n \leq 1$. Then, we can reduce the expression as follows.
Note that $1 - x \leq e^{-x}$. 

\begin{align*}
  \text{Pr[$e_j$ not selected]} &= \prod_{e_j \in S_i} ( 1 - 2x_i\text{ln}n ) \\
                            &\leq \prod_{e_j \in S_i} e^{-2x_i\text{ln}n} \\
                            &\leq e^{-2 \text{ln}n \times \sum_{e_j \in S_i} x_i} \\
                            &\leq e^{-2 \text{ln}n} \\
                            &\leq \frac{1}{n^2}
\end{align*}

In both cases, the following statements hold:

\begin{align*}
  \text{Pr[$e_j$ is selected]} &= 1 - \text{Pr[$e_j$ is not selected]} \\
                               &\geq 1 - \frac{1}{n^2}
\end{align*}

% Part C
\item We can generalize part B further. A feasible solution to the LP is one in which every $e_j$ is selected. 
This is the same as $1 - \text{Pr[there is a e not selected]}$. By the union bound, we can express the second half: 

\begin{align*}
  \text{Pr[There is a $e_j$ is not selected]} &\leq \sum_{e_j \in V} \text{Pr[$e_j$ is not selected]} \\
                                              &\leq n(1 - \frac{1}{n^2}) \\
                                              &\leq n - \frac{1}{n}
\end{align*}

Secondly, on the condition that the solution is feasible, we want to show that the expected cost is within 
O(ln$n$) times the objective value of the LP. We can use the same case analysis as earlier to break this down:

\begin{align*}
  \text{Obj(LP)} &= \sum_{i = 1}^m ( w_i \times x_i ) \\
  \mathbb{E}[cost] &= \sum_{i = 1}^m ( w_i \times \text{Pr[$S_i$ is selected]} ) \\
                   &= \sum_{i = 1}^m ( w_i \times \text{min}(1, 2x_i\text{ln}n)) \\
  \textbf{Case 1:} &\geq \sum_{i = 1}^m ( w_i \times 1 ) \\
  \textbf{Case 2:} &\leq \sum_{i = 1}^m ( w_i \times 2x_i\text{ln}n ) \\
                   &\leq 2\text{ln}n \times \sum_{i = 1}^m ( w_i \times x_i ) \\
                   &\leq 2\text{ln}n \times \text{Obj(LP)}
\end{align*}

\end{enumerate}

\end{document}
